---
title: "Stats 209 Final Project Code"
author: "Trent Bellinger"
date: "`r Sys.Date()`"
output: html_document
---

https://www.datalumos.org/datalumos/project/237384/version/V1/view?path=/datalumos/237384/fcr:versions/V1/Ethiopia/Improving-Quality-of-Primary-Education-Program--IQPEP-

```{r}
library(tidyverse)
library(DescTools)
library(estimatr)
library(fastDummies)
# read in the csv and choose the columns we will use
data <- read.csv('endline.csv', header = TRUE)
data <- data[, c("grade", "female", "age", "language", "region", "school_type", 
                 "encrypted_school_code", "treatment", "read_comp_score", "orf")]

#school_type is collinear with treatment. Have to drop it. Grouping all controls regardless of type under "control" makes this variable useless.
data <- data %>% select(-school_type)

# change the treatment variable to a binary integer
data$treatment <- ifelse(data$treatment == "Control", 0, 1)

# change school code to be 1,...,num_schools, remove original column
data$school_id <- as.numeric(factor(data$encrypted_school_code, levels = unique(data$encrypted_school_code)))
data <- data %>% select(-encrypted_school_code)

# convert character columns to factor
data[sapply(data, is.character)] <- lapply(data[sapply(data, is.character)], as.factor)

# remove rows with NAs
data <- na.omit(data)

# Age has a skewed distro with a massive outlier (someone aged 100). Will drop outlier as it seems like an error
data <- data[data$age <= 60, ]

# set up covariates (X), response (Y), and treatment (Z)
X <- data[, c("grade", "female", "age", "language", "region")] #language and region are colinear. Would remove one of them but kept it for now
Z <- data$treatment
Y <- data$read_comp_score # can also use orf (oral reading fluency)
```

# Baseline school-level analysis
```{r baseline}
school_level_data <- data %>% 
  group_by(school_id) %>% 
  summarise(
    orf = mean(orf, na.rm = TRUE), 
    r_comp_scr = mean(read_comp_score, na.rm = TRUE),
    z = mean(treatment, na.rm = TRUE)        
  )

y1 <- school_level_data$orf
y2 <- school_level_data$r_comp_scr
z <- school_level_data$z

model <- lm(y1 ~ z, data = school_level_data)
model_summary_orf <- summary(model)

model <- lm(y2 ~ z, data = school_level_data)
model_summary_r_comp <- summary(model)

print("--- ORF Results ---")
model_summary_orf$coefficients["z", ]

print("--- Reading Comp Results ---")
model_summary_r_comp$coefficients["z", ]
```
* Doing a simple analysis at the school level shows that there is only a statistically significant treatment effect on oral reading fluency, not reading comprehension. However, this didn't deploy any variance reduction techniques, which could render statistically significant results for both. 

# Grade-level simple analysis
```{r grade-level analysis}
grade_level_data <- data %>% 
  group_by(school_id,grade) %>% 
  summarise(
    orf = mean(orf, na.rm = TRUE), 
    r_comp_scr = mean(read_comp_score, na.rm = TRUE),
    z = mean(treatment, na.rm = TRUE),
    grade_3 = ifelse(first(grade)==3,1,0),
    .groups = "drop"
  )

y1 <- grade_level_data$orf
y2 <- grade_level_data$r_comp_scr
z <- grade_level_data$z

model <- lm_robust(y1 ~ z*grade_3, data = grade_level_data, clusters=school_id) ##clusters accounts for randomization at school level
model_summary_orf <- summary(model)

model <- lm_robust(y2 ~ z*grade_3, data = grade_level_data, clusters=school_id)
model_summary_r_comp <- summary(model)

print("--- ORF Results ---")
model_summary_orf$coefficients[c("z","z:grade_3"),1:4 ]

print("--- Reading Comp Results ---")
model_summary_r_comp$coefficients[c("z","z:grade_3"),1:4 ]
```
# school-level covariates
* Because our cluster experiment is at the school level, we need to aggregate the covariates at the school level.
* We will get the number of second graders, number of third graders, number of females, number of males, average age, standard deviation of age, and region of each school.

```{r aggregate covariates by school}
# collapse student covariates to school level for the propensity model
school_summary <- X %>%
  mutate(school_id = data$school_id) %>%
  group_by(school_id) %>%
  mutate(num_2grade = sum(grade == 2)) %>%
  mutate(num_3grade = sum(grade == 3)) %>%
  mutate(num_female = sum(female == "Female")) %>%
  mutate(num_male = sum(female == "Male")) %>%
  mutate(avg_age = mean(age)) %>%
  mutate(sd_age = sd(age)) %>%
  mutate(region = Mode(region)) %>%
  mutate(common_language = Mode(language)) %>%
  select(-grade, -female, -age, -language, -region) %>%
  distinct()
head(school_summary)
```

# school-level analysis with lin's estimator (variance reduction)
```{r lin estimator analysis}
##OHE common language
school_covars <- dummy_cols(school_summary, 
                   select_columns = "common_language", 
                   remove_first_dummy = TRUE)

#removing ids and common language
school_covars <- school_covars %>% 
  select(-common_language, -school_id)

#commented code below removes covariates that I suspect are modeling noise
#school_covars <- school_covars %>% 
#  select(-common_language, -school_id,-num_male,-num_female,-num_2grade,-num_3grade) 

# Center the covariates
school_covars_centered <- scale(school_covars, center = TRUE, scale = FALSE)

X_centered <- as.data.frame(school_covars_centered)

analysis_df <- data.frame(
  orf = school_level_data$orf,           # Y1
  read_score = school_level_data$r_comp_scr, # Y2
  z = school_level_data$z,               # Treatment
  X_centered                             # All centered covariates
)

#Create regression formula dynamically
covariate_names <- colnames(X_centered)
rhs_formula <- paste("z * (", paste(covariate_names, collapse = " + "), ")")

#Model for ORF (Y1)
formula_orf <- as.formula(paste("orf ~", rhs_formula))
lin_model_orf <- lm_robust(formula_orf, data = analysis_df)

#Model for Reading Score (Y2)
formula_read <- as.formula(paste("read_score ~", rhs_formula))
lin_model_read <- lm_robust(formula_read, data = analysis_df)

# results
print("--- Lin's Estimator Results (ORF) ---")
summary(lin_model_orf)$coefficients["z", ]

print("--- Lin's Estimator Results (Reading Score) ---")
summary(lin_model_read)$coefficients["z", ]
```
*The standard errors decreased utilizing Lin's estimator, showing it effectively reduced variance. However, the treatment effects and their corresponding p-values decreased. Now, the treatment effect on ORF isn't statistically significant. This is likely a result of randomly having better students in treatment vs control. 

# Student-level analysis with Lin's estimator
```{r lin estimator analysis}
##OHE language
X_enc <- dummy_cols(X, 
                   select_columns = "language", 
                   remove_first_dummy = TRUE)
X_enc$female <- ifelse(X_enc$female =="Female",1,0)
#keeping only covars
X_enc <- X_enc %>% 
  select(-language, -region) 

# Center the covariates
X_enc_cen <- scale(X_enc, center = TRUE, scale = FALSE)

X_centered <- as.data.frame(X_enc_cen)

analysis_df <- data.frame(
  orf = data$orf,                    # Y1
  read_score = data$read_comp_score, # Y2
  z = Z,                             # Treatment
  school_id = data$school_id,        # school id
  X_centered                         # All centered covariates
)

#Create regression formula dynamically
covariate_names <- colnames(X_centered)
rhs_formula <- paste("z * (", paste(covariate_names, collapse = " + "), ")")

#Model for ORF (Y1)
formula_orf <- as.formula(paste("orf ~", rhs_formula))
lin_model_orf <- lm_robust(formula_orf, data = analysis_df,clusters =school_id )

#Model for Reading Score (Y2)
formula_read <- as.formula(paste("read_score ~", rhs_formula))
lin_model_read <- lm_robust(formula_read, data = analysis_df,clusters =school_id)

# results
print("--- Lin's Estimator Results (ORF) student-level ---")
summary(lin_model_orf)$coefficients["z", ]

print("--- Lin's Estimator Results (Reading Score) student-level ---")
summary(lin_model_read)$coefficients["z", ]
```
*An analysis utilizing Lin's estimator at the student level showed that the intervention is statistically significant for both ORF and reading comprehension. The contrast with previous results is likely due to the fact that the aggregated covariates at the school level weren't very meaningful and allowed for noise in the model. Also in this set up with have a higher DF given lower amount of covariates. 

#AIPW utilizing causal forrests
*Causal forests handles cross fitting and hyper parameter tuning so me only need to train the model. It also builds the propensity score and outcome models simultaneously 
```{r causal_forest_training}
library(grf)
Y1<- data$orf
Y2<- data$read_comp_score
school <- data$school_id
cf1 <- causal_forest(
  X = X_centered, 
  Y = Y1, 
  W = Z,
  clusters = school, 
  tune.parameters = "all"
)
cf2 <- causal_forest(
  X = X_centered, 
  Y = Y2, 
  W = Z,
  clusters = school, 
  tune.parameters = "all"
)
```

```{r causal_forest_analysis}
print("----Results for ORF----")
ate_result_orf <- average_treatment_effect(cf1, target.sample = "all")
print(ate_result_orf)

#getting variables that have the greatest interaction with treatment
varimp <- variable_importance(cf1)
ranked_vars <-order(varimp, decreasing = TRUE)
colnames(X_centered)[ranked_vars]

#checking predictors of treatment effect
best_linear_projection(cf1, X_centered)

print("----Results for Reading Comp----")
ate_result_read_comp <- average_treatment_effect(cf2, target.sample = "all")
print(ate_result_read_comp)

#getting variables that have the greatest interaction with treatment
varimp <- variable_importance(cf2)
ranked_vars <-order(varimp, decreasing = TRUE)
colnames(X_centered)[ranked_vars]

#checking predictors of treatment effect
best_linear_projection(cf2, X_centered)
```
*Results are similar to those found utilizing lin's estimator at the student level. 
*HTE to note: Seems like the treatment is less effective across the board for the Sidamigna language. It also seems like the treatment has a greater effect on ORF for 3rd grade students. 

# Clusters with Doubly-Robust Estimand

* We conduct a cluster random experiment with school as the cluster.
* Each school is assigned to be treatment or control, so all units within the same cluster have the same treatment assignment.
* Denote each school as a variable $S$ where $S \in {1,...,K}$

```{r num_schools}
schools <- unique(data$school_id)
K <- length(schools)
paste0("Number of schools: ", K)
```

* We have 240 schools in the experiment, so $K = 240$.
* Just as a sanity check, we will now make sure that each school contains only treatment units or only control units.
* We will then check the number of treatment schoola and the number of control schools.

```{r school_treatment_control}
schools_treatment <- data %>% 
  group_by(school_id) %>% 
  mutate(num_students = n()) %>%
  mutate(num_treatment = sum(treatment)) %>% 
  mutate(num_control = n() - sum(treatment)) %>%
  select(school_id, num_students, num_treatment, num_control) %>%
  distinct() %>%
  mutate(treatment = as.numeric(num_treatment > 0))
head(schools_treatment)

num_control_schools <- sum(schools_treatment$num_control > 0)
paste0("Number of control schools: ", num_control_schools)

num_treatment_schools <- sum(schools_treatment$num_treatment > 0)
paste0("Number of treatment schools: ", num_treatment_schools)
```

* We have 120 treatment schools and 120 control schools.
* Denote $n_{[k]} = \#\{i:X_{i}=k\}$.
* Let $T_{k} \in \{0,1\}$, $k=1,...,K$, be the treatment indicator for each cluster.
* We create vectors to store the $n_{k}$'s, the $k$'s, and the $T_{k}$'s.

```{r cluster treatment vector}
nk_vec <- schools_treatment$num_students
k_vec <- schools_treatment$school_id
T_vec <- schools_treatment$treatment
```

*We can calculate the propensity score for each school using logistic regression.

```{r school-level ps}
# estimate school-level propensity score using logistic regression
ps_model <- glm(T_vec ~ . - school_id, data = school_summary, family = binomial)
e_hat_school <- ps_model$fitted.values
hist(e_hat_school)

# expand to student level
#e_hat_student <- e_hat_school[match(data$school_id, school_summary$school_id)]# dont think we can do this. 

ps_model_student <- glm(Z ~ . , data = X_enc, family = binomial)
e_hat_student <- ps_model_student$fitted.values
hist(e_hat_student)

```
* As expected, propensity scores are centered around 0.5 given the randomized setup. 

* Now that we have propensity scores for each school, we will create an outcome model on the student level using linear regression. We will evaluate that model on each student with $Z=1$ and $Z=0$ to get $\mu_{1}$ and $\mu_{0}$.

```{r outcome model}
# estimate outcome model using linear regression
out_mod <- lm(Y ~ Z + ., data = X)

data_Z1 <- X
data_Z1$Z <- 1
data_Z0 <- X
data_Z0$Z <- 0

mu1_hat <- predict(out_mod, newdata = data_Z1)
mu0_hat <- predict(out_mod, newdata = data_Z0)
```

* Now that we have propensity score and outcome model prediction fro each student/school, we can compute a doubly-robust AIPW estimand for each student. Our final estimand will be the mean of these values.

```{r AIPW tau_hat}
# AIPW score per student
aipw <- (Z * (Y - mu1_hat) / e_hat_student) - ((1 - Z) * (Y - mu0_hat) / (1 - e_hat_student)) + (mu1_hat - mu0_hat)
tau_hat <- mean(aipw)
paste0("tau_hat = ", tau_hat)
```

* We can now compute a variance estimate of our estimand.

```{r variance estimation}
# cluster-robust variance: aggregate student contributions per school
# Use cluster-level sums U_s = sum_{i in s} aipw_i
U_s <- tapply(aipw, data$school_id, sum)   # length S
S <- length(U_s)
U_bar <- mean(U_s)
N <-length(aipw)

# variance estimator: Var(tau_hat) = S/N^2 * Var_emp(U_s) where Var_emp uses (S-1) denom
var_hat <- (S / (N^2)) * (sum( (U_s - U_bar)^2) / (S - 1))
se_hat  <- sqrt(var_hat)
paste0("SE_hat = ", se_hat)
```

* Using this standard error approximation, we can compute a confidence interval.

```{r confidence interval}
# 95% CI
ci_lower <- tau_hat - qt(0.975, df = S - 1) * se_hat
ci_upper <- tau_hat + qt(0.975, df = S - 1) * se_hat

paste0("95% confidence interval: [", ci_lower, ", ", ci_upper, "]")
```

# Clusters with Diff-In-Means

```{r cluster diff-in-means}
schools <- unique(data$encrypted_school_code)
K <- length(schools)

Ybar_vec <- numeric(K)
k_vec <- numeric(K)
Z_k_vec <- numeric(K)

for (i in seq_along(schools)) {
  idx <- which(cluster == schools[i])
  k_vec[i] <- length(idx)
  Ybar_vec[i] <- mean(Y[idx])
  Z_k_vec[i] <- unique(Z[idx])   # assumes cluster-level treatment (same inside cluster)
}

# group counts
K_1 <- sum(Z_k_vec == 1)
K_0 <- sum(Z_k_vec == 0)

tau_hat <- mean(Ybar_vec[Z_k_vec == 1]) - mean(Ybar_vec[Z_k_vec == 0])

s1_sq <- var(Ybar_vec[Z_k_vec == 1])
s0_sq <- var(Ybar_vec[Z_k_vec == 0])
var_hat <- s1_sq / K_1 + s0_sq / K_0
se_hat  <- sqrt(var_hat)

# CI using t with G-2 df
df <- K - 2
tcrit <- qt(0.975, df)
CI <- c(tau_hat - tcrit * se_hat, tau_hat + tcrit * se_hat)

list(estimate = tau_hat, se = se_hat, df = df, CI = CI, num_clusters = K)
```

# Simple FRT with Diff-In-Means

```{r simple FRT with diff-in-means}
n <- length(Z)
n_0 <- sum(Z == 0)
n_1 <- sum(Z == 1)

# simple diff-in-means with FRT ignoring covariates
tau_hat <- mean(Y[Z == 1]) - mean(Y[Z == 0])

tau_hat_dist <- c()
B <- 10000
for (i in 1:B) {
  Z_resamp <- rep(0, n)
  Z_resamp[sample(1:n, n_1, replace = FALSE)] <- 1
  tau_hat_dist <- c(tau_hat_dist, mean(Y[Z_resamp == 1]) - mean(Y[Z_resamp == 0]))
}

p_value <- mean(tau_hat_dist > tau_hat)
p_value
```

# Matching

```{r macthing}
library(DOS2)
library(optmatch)
library(RItools)

matching_data <- data[, c("grade", "female", "age", "language", "region", "treatment", "read_comp_score")]

xBalance(treatment ~ grade + female + age + language + region, data = matching_data)

matching_data$pscore <- glm(treatment ~ grade + female + age + language + region, 
                            family = binomial, data = matching_data)$fitted.values

# ggplot(matching_data, aes(x = pscore, fill = factor(treatment))) +
#   geom_histogram(aes(y = after_stat(density)),
#                  alpha = 0.5, position = "identity", bins = 30) +
#   scale_fill_manual(values = c("blue", "red"),
#                     name = "Group",
#                     labels = c("Control", "Treatment")) +
#   labs(
#     title = "Distribution of Propensity Score by Treatment Group",
#     x = "Propensity Score",
#     y = "Count"
#   ) +
#   theme_minimal(base_size = 14)

options("optmatch_max_problem_size" = Inf)
match <- match_on(treatment ~ grade + female + age + language + region, data = matching_data)
ms <- pairmatch(match, data = matching_data)

# plot the covariate differences before and after matching
plot(xBalance(treatment ~ grade + female + age + language + region - 1, 
              strata = list(unstrat = NULL, ms.1 = ~ms), data = matching_data), ggplot = TRUE)

matching_data <- matching_data %>% mutate(pair_id = ms)
paste0("Proportion of matched units = ", mean(is.na(matching_data$pair_id)))

# pair_ps <- matching_data %>%
#   mutate(pair_id = ms) %>%
#   filter(!is.na(pair_id)) %>%
#   group_by(pair_id) %>%
#   summarise(
#     treat_ps = pscore[treatment == 1],
#     control_ps = pscore[treatment == 0],
#     abs_diff = abs(treat_ps - control_ps)
#   )
# 
# # Compute average and maximum absolute difference
# avg_abs_diff_before <- mean(pair_ps$abs_diff, na.rm = TRUE)
# max_abs_diff_before <- max(pair_ps$abs_diff, na.rm = TRUE)
# 
# cat("Average absolute PS difference =", round(avg_abs_diff_before, 4), "\n")
# cat("Maximum absolute PS difference =", round(max_abs_diff_before, 4), "\n")
# 
# match_caliper <- addcaliper(match, z = matching_data$treatment, p = matching_data$pscore, 
#                             caliper = 0.1 * sd(matching_data$pscore, na.rm = TRUE))
# ms_cal <- pairmatch(match_caliper, data = matching_data)
# 
# # plot the covariate differences before and after matching
# plot(xBalance(treatment ~ grade + female + age + language + region - 1, 
#               strata = list(unstrat = NULL, ms.1 = ~ms_cal), data = matching_data), ggplot = TRUE)
# 
# # compute new pairwise PS differences after caliper matching
# pair_ps_cal <- matching_data %>%
#   mutate(pair_id = ms_cal) %>%
#   filter(!is.na(pair_id)) %>%
#   group_by(pair_id) %>%
#   summarise(
#     treat_ps = pscore[treatment == 1],
#     control_ps = pscore[treatment == 0],
#     abs_diff = abs(treat_ps - control_ps)
#   )
# 
# # Average and maximum differences
# avg_abs_diff_after <- mean(pair_ps_cal$abs_diff, na.rm = TRUE)
# max_abs_diff_after <- max(pair_ps_cal$abs_diff, na.rm = TRUE)
# 
# cat("Average absolute PS difference after caliper =", round(avg_abs_diff_after, 4), "\n")
# cat("Maximum absolute PS difference after caliper =", round(max_abs_diff_after, 4), "\n")

matched_data <- matching_data %>% filter(!is.na(pair_id))

# get the difference in y for each pairing
diff_vec <- c()
for (pair in unique(matched_data$pair_id)) {
  treatment_y <- matched_data[(matched_data$pair_id == pair) & (matched_data$treatment == 1), 'read_comp_score']
  control_y <- matched_data[(matched_data$pair_id == pair) & (matched_data$treatment == 0), 'read_comp_score']
  diff_vec <- c(diff_vec, treatment_y - control_y)
}
tau_hat <- mean(diff_vec)

# Fisher randomization test
set.seed(42)
B <- 1000
sampled_tau_hats <- c()
for (b in 1:B) {
  # randomly change treatment/control for some matched pairs
  random_signs <- sample(c(-1, +1), size = length(diff_vec), replace = TRUE)
  diff_vec_resamp <- diff_vec * random_signs
  sampled_tau_hats <- c(sampled_tau_hats, mean(diff_vec_resamp))
}

p_value <- mean(abs(sampled_tau_hats) >= abs(tau_hat))
cat("p-value =", p_value)

tau_hat
```




